# ğŸš€ AWS Serverless Data Pipeline (S3 â†’ Lambda â†’ Glue â†’ Athena â†’ QuickSight)
*A fully automated, beginner-friendly, production-style data engineering pipeline.*

This project processes CSV files uploaded into S3, transforms them using AWS Lambda, catalogs them using AWS Glue, queries them with Amazon Athena, and visualizes them through Amazon QuickSight using **Direct Query**.

This README is designed to be **very simple** â€” spoon-feeding style â€” so even a complete beginner can follow confidently.

---

#  ğŸŒŸ 1. Architecture Overview  

```
Upload CSV â†’ S3 Raw Folder  
        â†“  
Lambda Trigger (Transforms CSV)  
        â†“  
S3 Processed Folder  
        â†“  
Glue Crawler Creates Schema  
        â†“  
Athena SQL Query  
        â†“  
QuickSight Dashboard (Direct Query)
```

![Architecture Diagram](./screenshots/architecture_diagram.png)

---

# ğŸ—‚ 2. Project Structure  

```
aws-data-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample_small.csv
â”‚   â””â”€â”€ orders_large_10k.csv
â”‚
â”œâ”€â”€ lambda/
â”‚   â””â”€â”€ process_file.py
â”‚
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ s3_raw.png
â”‚   â”œâ”€â”€ s3_processed.png
â”‚   â”œâ”€â”€ lambda_function.png
â”‚   â”œâ”€â”€ glue_crawler.png
â”‚   â”œâ”€â”€ athena_preview.png
â”‚   â”œâ”€â”€ dashboard.png
â”‚   â””â”€â”€ architecture_diagram.png
â”‚
â””â”€â”€ README.md
```

---

#  ğŸ§  3. Prerequisites

Before you begin, make sure you have:

- AWS Account  
- IAM User with **AdministratorAccess**  
- Region set to **ap-south-1 (Mumbai)**  
- Basic AWS Console knowledge  

---

#  ğŸ½ 4. Step-by-Step Setup (Beginner Friendly)

---

# â­ STEP 1 â€” Create S3 Bucket Structure

Create a bucket named:

```
vinit-data-pipeline
```

Inside it, create:

```
raw/
processed/
```

### ğŸ“¸ Screenshot â€” S3 Raw Folder  
<img width="1920" height="1080" alt="Image" src="https://github.com/user-attachments/assets/393f3354-582b-4a50-9845-7b91c2006303" />

### ğŸ“¸ Screenshot â€” S3 Processed Folder  
*(After Lambda runs)*  
<img width="1920" height="1080" alt="Image" src="https://github.com/user-attachments/assets/aba2deee-44bc-4874-93e9-32641512b1a4" />

---

# â­ STEP 2 â€” Create Lambda Function

Go to:  
AWS Console â†’ **Lambda â†’ Create function**

| Setting | Value |
|--------|--------|
| Name | vinit-process-lambda |
| Runtime | Python 3.x |
| Role | Create new role |

### Paste this code:

```python
import boto3
import csv
import io

def lambda_handler(event, context):
    s3 = boto3.client("s3")

    raw_bucket = "vinit-data-pipeline"
    processed_bucket = "vinit-data-pipeline"

    for record in event['Records']:
        key = record['s3']['object']['key']

        if "raw/" not in key:
            return

        obj = s3.get_object(Bucket=raw_bucket, Key=key)
        data = obj['Body'].read().decode('utf-8').splitlines()
        reader = csv.reader(data)

        header = next(reader)
        processed_rows = list(reader)

        output = io.StringIO()
        writer = csv.writer(output)
        writer.writerow(header)
        writer.writerows(processed_rows)

        processed_key = key.replace("raw/", "processed/")
        s3.put_object(
            Bucket=processed_bucket,
            Key=processed_key,
            Body=output.getvalue()
        )

        print("Processed:", processed_key)
```

### â­ Add S3 Trigger:
- Bucket: `vinit-data-pipeline`
- Prefix: `raw/`
- Event: **PUT**

### ğŸ“¸ Screenshot â€” Lambda Function  
<img width="1920" height="1080" alt="Image" src="https://github.com/user-attachments/assets/fbd9eddc-00ca-408e-a786-56d517de379e" />

---

# â­ STEP 3 â€” Upload CSV File

Upload:

```
orders_large_10k.csv
```

into:

```
vinit-data-pipeline/raw/
```

Lambda will:

âœ” Read the file  
âœ” Process it  
âœ” Write to `/processed/`

---

# â­ STEP 4 â€” Create Glue Crawler

**AWS Glue â†’ Crawlers â†’ Create Crawler**

| Option | Value |
|--------|--------|
| Source | S3 |
| S3 Path | s3://vinit-data-pipeline/processed/ |
| IAM Role | AWSGlueServiceRole |
| Database | vinit_db |
| Schedule | On Demand |

Run the crawler â†’ It creates **processed** table.

### ğŸ“¸ Screenshot â€” Glue Crawler Completed  
<img width="1920" height="1080" alt="Image" src="https://github.com/user-attachments/assets/b574c411-5e3b-4a75-9955-1f8b1ef75e52" />

---

# â­ STEP 5 â€” Query Data in Athena

Go to Athena â†’ Query Editor  
Select:

- Catalog: **AwsDataCatalog**  
- Database: **vinit_db**  

### Run query:

```sql
SELECT * FROM processed LIMIT 10;
```

### ğŸ“¸ Screenshot â€” Athena Preview  
<img width="1920" height="1080" alt="Image" src="https://github.com/user-attachments/assets/cc20d01f-d6d8-48a5-9d42-379129d4e9cd" />

---

# â­ STEP 6 â€” QuickSight Dashboard Setup

Open QuickSight â†’ Datasets â†’ **New Dataset**

Select:

- Source: Athena  
- Database: `vinit_db`  
- Table: `processed`  
- Mode: **Direct Query**

---

# â­ STEP 7 â€” Build Visual Dashboard

### Visuals to create:

#### 1ï¸âƒ£ Donut Chart  
- Group: product_category  
- Value: average(amount)  

#### 2ï¸âƒ£ Line Chart  
- X-axis: order_date  
- Value: sum(amount)

#### 3ï¸âƒ£ Bar Chart  
- X-axis: product_category  
- Value: count(order_id)

#### 4ï¸âƒ£ Horizontal Bar Chart  
- Y-axis: customer_id  
- Value: sum(amount)

#### KPIs:
- Total Revenue  
- Total Orders  
- Average Order Amount (calculated field)

Finally publish:

```
Vinit Sales Dashboard
```

---

#  ğŸ‘¨â€ğŸ’» Author

**Vinit Tippanawar**  
AWS | Cloud | Data Engineering Enthusiast  

If you like this project, please â­ star the repo!

